[id="troubleshooting"]
= Troubleshooting
:copyright: This file is part of korrel8r, released under https://github.com/korrel8r/korrel8r/blob/main/LICENSE
:icons: font
// Metadata
:keywords: correlation, observability, resource, signal, kubernetes
:description: Correlation of observability signal data
// URLs
:github: https://github.com/korrel8r/
:project: {github}/operator#readme
:quay-operator: quay.io/korrel8r/operator
// abbreviations
:red-hat: Red{nbsp}Hat
:rh-ocp: {red-hat} OpenShift Container Platform
:rh-console: link:https://docs.openshift.com/container-platform/latest/web_console/web-console-overview.html[{rh-ocp} web console]
:logging: logging subsystem for {red-hat} OpenShift
:operator: Korrel8r Community Operator

[id="troubleshooting-ocp-415-errors"]
== Installation errors on {rh-ocp} 4.15

{rh-ocp} 4.15 enforces additional security restrictions that did were not included in {rh-ocp} 4.14 and earlier versions.
Installing the Operator bundle on {rh-ocp} 4.15 causes security policy errors.

You can use the following workarounds:

* Deploy Korrel8r in the `default` namespace instead of creating a new `korrel8r` namespace, by running the following command:
+
[source,terminal]
----
$ operator-sdk -n default run bundle quay.io/korrel8r/operator-bundle:latest
----

* Apply labels to the `korrel8r` namespace before installing, by running the following commands:
+
[source,terminal]
----
$ kubectl label ns/korrel8r pod-security.kubernetes.io/enforce=privileged --overwrite
----
+
[source,terminal]
----
$ kubectl label ns/korrel8r pod-security.kubernetes.io/warn=privileged --overwrite
----

See also https://issues.redhat.com/browse/OU-304.

[id="troubleshooting-no-related-logs"]
== There is no "see related logs" link

The *see related logs* link does not appear unless the following criteria is met:

. The alert is _related_ to a container workload.
. The workload has generated logs.
. The logs have been collected by the {logging}.

For example, the `UpdateAvailable` alert indicates an update is available for the entire cluster, it is not related to any specific workload.

You can force the creation of an alert with *see related logs* by using the following procedure.

.Procedure

. Run the following command to create a broken deployment in a system namespace:
+
[source,terminal]
----
kubectl apply -f - << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-deployment
  namespace: default <1>
spec:
  selector:
    matchLabels:
      app: bad-deployment
  template:
    metadata:
      labels:
        app: bad-deployment
    spec:
      containers: <2>
      - name: bad-deployment
      	image: quay.io/openshift-logging/vector:5.8
----
<1> The deployment must be in a system namespace (such as `default`) to cause the desired alerts.
<2> This container deliberately tries to start a `vector` server with no configuration file. The server will log a few messages, and then exit with an error. Any container could be used for this.

. View the alerts:
.. Go to *Observe* -> *Alerting* and click *clear all filters*. View the `Pending` alerts.
+
[IMPORTANT]
====
Alerts first appear in the `Pending` state. They do not start `Firing` until the container has been crashing for some time. By showing `Pending` alerts you can see them much more quickly.
====
.. Look for `KubeContainerWaiting` alerts, `KubePodCrashLooping` alerts, or `KubePodNotReady` alerts. These alerts have a *show related logs* link.

[id="troubleshooting-no-logging-system-logs"]
== There are no logs for the {logging}

The log collector does not collect its own logs. Doing so might create a circular condition where collecting its own logs causes the collector to log something, which it then collects, which causes it to log something, and so on in an endless cycle.

To avoid this risk, the log collector does not collect any logs from pods that are part of the logging system.

You can still see view {logging} logs by using the `kubectl logs` command directly.
